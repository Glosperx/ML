{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95335a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Analiză Time Series - Energie Cinetică Pentilfuran cu FFT\n",
    "# \n",
    "# Acest notebook analizează datele de energie cinetică pentru molecula de pentilfuran într-un câmp electric.\n",
    "# Folosim transformata Fourier pentru a exploata natura sinusoidală a datelor.\n",
    "# \n",
    "# **Parametri cunoscuți:**\n",
    "# - Frecvența dominantă: 0.020000 Hz\n",
    "# - Perioada principală: 50.00 pași de timp\n",
    "# - Window size recomandat: 100 pași\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Import Libraries și Configurare\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq, ifft\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurare pentru reproducibilitate\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"📦 Libraries importate cu succes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684946fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 2. Încărcarea și Preprocesarea Datelor\n",
    "\n",
    "# %%\n",
    "# Încărcare date\n",
    "my_file = \"./pentilfuran.MDE\"\n",
    "\n",
    "df = pd.read_csv(\n",
    "    my_file,\n",
    "    sep=r\"\\s+\",\n",
    "    comment='#',\n",
    "    names=[\"Step\", \"T\", \"E_KS\", \"E_tot\", \"Vol\", \"P\"]\n",
    ")\n",
    "\n",
    "print(f\"📊 Dimensiune date originale: {df.shape}\")\n",
    "print(f\"📋 Coloane disponibile: {df.columns.tolist()}\")\n",
    "print(f\"🔢 Numărul de steps unici: {df['Step'].nunique()}\")\n",
    "\n",
    "# %%\n",
    "# Selectare liniile 1:901 pentru fiecare Step (optimizat)\n",
    "df_data = (\n",
    "    df.groupby(\"Step\", group_keys=False)\n",
    "    .apply(lambda g: g.iloc[1:901])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"✅ Dimensiunea după filtrare: {len(df_data)} rânduri\")\n",
    "print(f\"📈 Range energie totală: [{df_data['E_tot'].min():.6f}, {df_data['E_tot'].max():.6f}]\")\n",
    "print(f\"🌡️ Range temperatură: [{df_data['T'].min():.4f}, {df_data['T'].max():.4f}]\")\n",
    "\n",
    "# Verificare pentru valori lipsă\n",
    "print(f\"\\n🔍 Valori lipsă per coloană:\")\n",
    "print(df_data.isnull().sum())\n",
    "\n",
    "# %% [markdown]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27cde6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3. Analiza Periodicității cu FFT\n",
    "\n",
    "# %%\n",
    "def analyze_periodicity(signal_data, sampling_rate=1.0, plot=True):\n",
    "    \"\"\"Analiză avansată de periodicitate cu FFT\"\"\"\n",
    "    print(\"🔍 Analiză periodicitate cu FFT...\")\n",
    "    \n",
    "    # Remove trend pentru FFT mai precisă\n",
    "    detrended_signal = signal.detrend(signal_data)\n",
    "    \n",
    "    # FFT\n",
    "    fft_values = fft(detrended_signal)\n",
    "    frequencies = fftfreq(len(signal_data), d=1/sampling_rate)\n",
    "    \n",
    "    # Power spectrum (doar frecvențele pozitive, fără DC)\n",
    "    power_spectrum = np.abs(fft_values[1:len(signal_data)//2])\n",
    "    freqs_positive = frequencies[1:len(signal_data)//2]\n",
    "    \n",
    "    # Top 10 frecvențe dominante\n",
    "    dominant_indices = np.argsort(power_spectrum)[-10:][::-1]\n",
    "    dominant_freqs = freqs_positive[dominant_indices]\n",
    "    dominant_powers = power_spectrum[dominant_indices]\n",
    "    \n",
    "    print(f\"🎯 Top 10 frecvențe dominante:\")\n",
    "    for i, (freq, power) in enumerate(zip(dominant_freqs, dominant_powers)):\n",
    "        period = 1/freq if freq != 0 else np.inf\n",
    "        print(f\"   {i+1:2d}. Freq: {freq:.6f} Hz, Perioadă: {period:8.2f} pași, Putere: {power:.2e}\")\n",
    "    \n",
    "    if plot:\n",
    "        # Plot FFT\n",
    "        fig = sp.make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            subplot_titles=['Semnal Original vs Detrended', 'Power Spectrum (FFT)']\n",
    "        )\n",
    "        \n",
    "        # Semnal original vs detrended (primele 1000 puncte pentru vizibilitate)\n",
    "        sample_size = min(1000, len(signal_data))\n",
    "        x_axis = np.arange(sample_size)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=x_axis, y=signal_data[:sample_size], \n",
    "                      name='Original', line=dict(color='blue')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=x_axis, y=detrended_signal[:sample_size], \n",
    "                      name='Detrended', line=dict(color='red')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Power spectrum (zoom pe primele 50 frecvențe pentru claritate)\n",
    "        freq_limit = min(50, len(freqs_positive))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=freqs_positive[:freq_limit], y=power_spectrum[:freq_limit],\n",
    "                      mode='lines+markers', name='Power Spectrum'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Evidențiază frecvențele dominante\n",
    "        for i, (freq, power) in enumerate(zip(dominant_freqs[:5], dominant_powers[:5])):\n",
    "            if freq <= freqs_positive[freq_limit-1]:  # Doar dacă e în range-ul vizualizat\n",
    "                fig.add_vline(x=freq, line_dash=\"dash\", \n",
    "                             annotation_text=f\"f{i+1}={freq:.4f}Hz\", \n",
    "                             row=2, col=1)\n",
    "        \n",
    "        fig.update_layout(height=800, title_text=\"Analiză FFT - Energie Totală\")\n",
    "        fig.update_xaxes(title_text=\"Timp (pași)\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"Frecvența (Hz)\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Energie\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Amplitudine\", row=2, col=1)\n",
    "        fig.show()\n",
    "    \n",
    "    return dominant_freqs, power_spectrum, frequencies, detrended_signal\n",
    "\n",
    "# Rulare analiză FFT pe energia totală\n",
    "energy_data = df_data['E_tot'].values\n",
    "dominant_freqs, power_spectrum, frequencies, detrended_energy = analyze_periodicity(energy_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe790ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Crearea Features Fourier\n",
    "\n",
    "# %%\n",
    "def create_fourier_features(data, dominant_freqs, top_n=5):\n",
    "    \"\"\"Creează features bazate pe componentele Fourier dominante\"\"\"\n",
    "    print(f\"🌊 Creare {top_n} features Fourier...\")\n",
    "    \n",
    "    fourier_features = []\n",
    "    feature_names = []\n",
    "    \n",
    "    t = np.arange(len(data))\n",
    "    \n",
    "    for i, freq in enumerate(dominant_freqs[:top_n]):\n",
    "        # Sin și Cos pentru fiecare frecvență dominantă\n",
    "        sin_component = np.sin(2 * np.pi * freq * t)\n",
    "        cos_component = np.cos(2 * np.pi * freq * t)\n",
    "        \n",
    "        fourier_features.extend([sin_component, cos_component])\n",
    "        feature_names.extend([f'sin_f{i+1}_{freq:.4f}Hz', f'cos_f{i+1}_{freq:.4f}Hz'])\n",
    "    \n",
    "    print(f\"✅ Features Fourier create: {feature_names}\")\n",
    "    return np.array(fourier_features).T, feature_names\n",
    "\n",
    "# Crearea features Fourier\n",
    "fourier_features, fourier_names = create_fourier_features(energy_data, dominant_freqs, top_n=3)\n",
    "print(f\"📏 Shape features Fourier: {fourier_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0473b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Feature Engineering Complet\n",
    "\n",
    "# %%\n",
    "def create_comprehensive_features(df_data, fourier_features, fourier_names):\n",
    "    \"\"\"Creează un set complet de features pentru training\"\"\"\n",
    "    print(\"🔧 Creare features comprehensive...\")\n",
    "    \n",
    "    # Scalere pentru diferite tipuri de date\n",
    "    energy_scaler = StandardScaler()\n",
    "    temp_scaler = StandardScaler()\n",
    "    pressure_scaler = StandardScaler()\n",
    "    \n",
    "    # Features de bază scalate\n",
    "    energy_scaled = energy_scaler.fit_transform(df_data[['E_tot']]).flatten()\n",
    "    temp_scaled = temp_scaler.fit_transform(df_data[['T']]).flatten()\n",
    "    pressure_scaled = pressure_scaler.fit_transform(df_data[['P']]).flatten()\n",
    "    \n",
    "    # Features derivate pentru energie\n",
    "    energy_diff = np.gradient(energy_scaled)\n",
    "    energy_diff2 = np.gradient(energy_diff)  # Accelerație\n",
    "    \n",
    "    # Moving averages (pentru capturarea trend-urilor)\n",
    "    energy_ma5 = pd.Series(energy_scaled).rolling(window=5, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "    energy_ma10 = pd.Series(energy_scaled).rolling(window=10, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "    energy_ma20 = pd.Series(energy_scaled).rolling(window=20, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "    \n",
    "    # Volatilitate (rolling std)\n",
    "    energy_vol5 = pd.Series(energy_scaled).rolling(window=5, center=True).std().fillna(0)\n",
    "    energy_vol10 = pd.Series(energy_scaled).rolling(window=10, center=True).std().fillna(0)\n",
    "    \n",
    "    # Features pentru temperatură\n",
    "    temp_diff = np.gradient(temp_scaled)\n",
    "    temp_ma10 = pd.Series(temp_scaled).rolling(window=10, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "    \n",
    "    # Combinare toate features\n",
    "    all_features = np.column_stack([\n",
    "        energy_scaled,              # Target principal\n",
    "        temp_scaled,                # Temperatură\n",
    "        pressure_scaled,            # Presiune\n",
    "        energy_diff,                # Viteza energiei\n",
    "        energy_diff2,               # Accelerația energiei\n",
    "        energy_ma5,                 # Trend pe termen scurt\n",
    "        energy_ma10,                # Trend pe termen mediu\n",
    "        energy_ma20,                # Trend pe termen lung\n",
    "        energy_vol5,                # Volatilitate scurtă\n",
    "        energy_vol10,               # Volatilitate medie\n",
    "        temp_diff,                  # Rata schimbării temperaturii\n",
    "        temp_ma10,                  # Trend temperatură\n",
    "        fourier_features           # Features Fourier\n",
    "    ])\n",
    "    \n",
    "    # Namen features\n",
    "    feature_names = [\n",
    "        'Energy_scaled', 'Temp_scaled', 'Pressure_scaled',\n",
    "        'Energy_velocity', 'Energy_acceleration',\n",
    "        'Energy_MA5', 'Energy_MA10', 'Energy_MA20',\n",
    "        'Energy_Vol5', 'Energy_Vol10',\n",
    "        'Temp_velocity', 'Temp_MA10'\n",
    "    ] + fourier_names\n",
    "    \n",
    "    print(f\"✅ Total features: {all_features.shape[1]}\")\n",
    "    print(f\"📋 Feature names: {feature_names}\")\n",
    "    \n",
    "    return all_features, feature_names, {\n",
    "        'energy_scaler': energy_scaler,\n",
    "        'temp_scaler': temp_scaler,\n",
    "        'pressure_scaler': pressure_scaler\n",
    "    }\n",
    "\n",
    "# Creare features complete\n",
    "all_features, feature_names, scalers = create_comprehensive_features(df_data, fourier_features, fourier_names)\n",
    "\n",
    "# Verificare pentru NaN sau Inf\n",
    "print(f\"\\n🔍 Verificare calitatea features:\")\n",
    "print(f\"NaN values: {np.isnan(all_features).sum()}\")\n",
    "print(f\"Inf values: {np.isinf(all_features).sum()}\")\n",
    "print(f\"Feature range: [{all_features.min():.4f}, {all_features.max():.4f}]\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Crearea Secvențelor pentru LSTM\n",
    "\n",
    "# %%\n",
    "def create_sequences_optimized(features, energy_target, sequence_length, out_steps, overlap_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Creează secvențe optimizate cu overlap pentru mai multe date de training\n",
    "    \n",
    "    Args:\n",
    "        features: Array cu toate features (include target la coloana 0)\n",
    "        energy_target: Target-ul pentru predicție (energia scalată)\n",
    "        sequence_length: Lungimea secvenței input\n",
    "        out_steps: Numărul de pași de prezis\n",
    "        overlap_ratio: Raportul de overlap între secvențe (0.8 = 80% overlap)\n",
    "    \"\"\"\n",
    "    print(f\"🔧 Creare secvențe cu parametri:\")\n",
    "    print(f\"   - Sequence length: {sequence_length}\")\n",
    "    print(f\"   - Output steps: {out_steps}\")\n",
    "    print(f\"   - Overlap ratio: {overlap_ratio}\")\n",
    "    \n",
    "    sequences, targets = [], []\n",
    "    \n",
    "    # Calculează pas-ul bazat pe overlap\n",
    "    step_size = max(1, int(sequence_length * (1 - overlap_ratio)))\n",
    "    \n",
    "    # Generează secvențe cu overlap\n",
    "    for i in range(0, len(features) - sequence_length - out_steps + 1, step_size):\n",
    "        # Input sequence (toate features)\n",
    "        seq = features[i:i + sequence_length]\n",
    "        \n",
    "        # Target sequence (doar energia)\n",
    "        target = energy_target[i + sequence_length:i + sequence_length + out_steps]\n",
    "        \n",
    "        if len(target) == out_steps:  # Verifică că target-ul e complet\n",
    "            sequences.append(seq)\n",
    "            targets.append(target)\n",
    "    \n",
    "    sequences = np.array(sequences)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    print(f\"✅ Secvențe create:\")\n",
    "    print(f\"   - Input shape: {sequences.shape}\")\n",
    "    print(f\"   - Target shape: {targets.shape}\")\n",
    "    print(f\"   - Total samples: {len(sequences)}\")\n",
    "    \n",
    "    return sequences, targets\n",
    "\n",
    "# Parametrii optimizați pentru perioada de 50 pași\n",
    "SEQUENCE_LENGTH = 100  # 2x perioada principală\n",
    "OUT_STEPS = 25         # 0.5x perioada pentru predicții precise\n",
    "OVERLAP_RATIO = 0.7    # 70% overlap pentru mai multe sample-uri\n",
    "\n",
    "# Crearea secvențelor\n",
    "energy_target = all_features[:, 0]  # Prima coloană e energia scalată\n",
    "sequences, targets = create_sequences_optimized(\n",
    "    all_features, \n",
    "    energy_target, \n",
    "    SEQUENCE_LENGTH, \n",
    "    OUT_STEPS,\n",
    "    OVERLAP_RATIO\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Construirea Modelului LSTM Avansat\n",
    "\n",
    "# %%\n",
    "def build_advanced_lstm_model(input_shape, out_steps, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Construiește model LSTM avansat optimizat pentru date periodice\n",
    "    \"\"\"\n",
    "    print(f\"🏗️ Construire model LSTM pentru:\")\n",
    "    print(f\"   - Input shape: {input_shape}\")\n",
    "    print(f\"   - Output steps: {out_steps}\")\n",
    "    print(f\"   - Dropout rate: {dropout_rate}\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        # Layer 1: Bidirectional LSTM pentru capturarea dependințelor în ambele direcții\n",
    "        Bidirectional(\n",
    "            LSTM(128, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate),\n",
    "            input_shape=input_shape,\n",
    "            name='bidirectional_lstm_1'\n",
    "        ),\n",
    "        BatchNormalization(name='batch_norm_1'),\n",
    "        \n",
    "        # Layer 2: Al doilea LSTM bidirectional\n",
    "        Bidirectional(\n",
    "            LSTM(64, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate),\n",
    "            name='bidirectional_lstm_2'\n",
    "        ),\n",
    "        BatchNormalization(name='batch_norm_2'),\n",
    "        \n",
    "        # Layer 3: LSTM final\n",
    "        LSTM(32, dropout=dropout_rate, recurrent_dropout=dropout_rate, name='lstm_final'),\n",
    "        BatchNormalization(name='batch_norm_3'),\n",
    "        \n",
    "        # Dense layers cu regularizare\n",
    "        Dense(64, activation='relu', name='dense_1'),\n",
    "        Dropout(dropout_rate + 0.1, name='dropout_1'),\n",
    "        \n",
    "        Dense(32, activation='relu', name='dense_2'),\n",
    "        Dropout(dropout_rate, name='dropout_2'),\n",
    "        \n",
    "        Dense(16, activation='relu', name='dense_3'),\n",
    "        Dropout(dropout_rate * 0.5, name='dropout_3'),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(out_steps, activation='linear', name='output')\n",
    "    ])\n",
    "    \n",
    "    # Optimizer cu parametri optimizați\n",
    "    optimizer = Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7\n",
    "    )\n",
    "    \n",
    "    # Compilare cu loss function robust\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='huber',  # Mai robust la outliers decât MSE\n",
    "        metrics=['mae', 'mse', 'mape']\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Model construit cu succes!\")\n",
    "    print(f\"📊 Parametri totali: {model.count_params():,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Construire model\n",
    "model = build_advanced_lstm_model(\n",
    "    input_shape=(sequences.shape[1], sequences.shape[2]),\n",
    "    out_steps=OUT_STEPS,\n",
    "    dropout_rate=0.3\n",
    ")\n",
    "\n",
    "# Afișare arhitectura modelului\n",
    "model.summary()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Împărțirea Datelor și Antrenarea\n",
    "\n",
    "# %%\n",
    "# Split date (fără shuffle pentru time series)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sequences, targets, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=False  # Important pentru time series\n",
    ")\n",
    "\n",
    "print(f\"📊 Împărțirea datelor:\")\n",
    "print(f\"   - Train samples: {X_train.shape[0]}\")\n",
    "print(f\"   - Test samples: {X_test.shape[0]}\")\n",
    "print(f\"   - Features per sample: {X_train.shape[2]}\")\n",
    "print(f\"   - Sequence length: {X_train.shape[1]}\")\n",
    "\n",
    "# Definire callbacks avansate\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        min_delta=1e-6\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1,\n",
    "        cooldown=5\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_energy_lstm_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "        save_weights_only=False\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"🚀 Începe antrenarea...\")\n",
    "\n",
    "# %% \n",
    "# Antrenare model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"✅ Antrenarea completată!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Evaluarea Modelului\n",
    "\n",
    "# %%\n",
    "def evaluate_comprehensive(model, X_test, y_test, scalers, feature_names):\n",
    "    \"\"\"Evaluare comprehensivă a modelului\"\"\"\n",
    "    print(\"📈 Evaluare model...\")\n",
    "    \n",
    "    # Predicții\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Scalare inversă pentru metrici în unități originale\n",
    "    energy_scaler = scalers['energy_scaler']\n",
    "    \n",
    "    # Flatten pentru calcularea metricilor\n",
    "    y_test_flat = y_test.flatten().reshape(-1, 1)\n",
    "    y_pred_flat = y_pred.flatten().reshape(-1, 1)\n",
    "    \n",
    "    # Scalare inversă\n",
    "    y_test_original = energy_scaler.inverse_transform(y_test_flat).flatten()\n",
    "    y_pred_original = energy_scaler.inverse_transform(y_pred_flat).flatten()\n",
    "    \n",
    "    # Calculare metrici\n",
    "    mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "    mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "    r2 = r2_score(y_test_original, y_pred_original)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # MAPE (Mean Absolute Percentage Error)\n",
    "    mape = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100\n",
    "    \n",
    "    print(f\"🏆 Performanța modelului:\")\n",
    "    print(f\"   ├── MSE: {mse:.8f}\")\n",
    "    print(f\"   ├── MAE: {mae:.8f}\")\n",
    "    print(f\"   ├── RMSE: {rmse:.8f}\")\n",
    "    print(f\"   ├── R²: {r2:.6f}\")\n",
    "    print(f\"   └── MAPE: {mape:.4f}%\")\n",
    "    \n",
    "    # Analiza reziduurilor\n",
    "    residuals = y_pred_original - y_test_original\n",
    "    print(f\"\\n📊 Analiza reziduurilor:\")\n",
    "    print(f\"   ├── Mean residual: {np.mean(residuals):.8f}\")\n",
    "    print(f\"   ├── Std residual: {np.std(residuals):.8f}\")\n",
    "    print(f\"   ├── Min residual: {np.min(residuals):.8f}\")\n",
    "    print(f\"   └── Max residual: {np.max(residuals):.8f}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': y_pred,\n",
    "        'predictions_original': y_pred_original,\n",
    "        'targets_original': y_test_original,\n",
    "        'residuals': residuals,\n",
    "        'metrics': {\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mape': mape\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Evaluare model\n",
    "results = evaluate_comprehensive(model, X_test, y_test, scalers, feature_names)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Vizualizări Complete\n",
    "\n",
    "# %%\n",
    "def create_comprehensive_plots(results, history):\n",
    "    \"\"\"Creează vizualizări complete pentru analiza modelului\"\"\"\n",
    "    print(\"📊 Creare vizualizări...\")\n",
    "    \n",
    "    # Extrage rezultatele\n",
    "    y_pred_orig = results['predictions_original']\n",
    "    y_test_orig = results['targets_original']\n",
    "    residuals = results['residuals']\n",
    "    metrics = results['metrics']\n",
    "    \n",
    "    # 1. Plot principal cu 4 subplots\n",
    "    fig = sp.make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[\n",
    "            'Predicții vs Realitate', \n",
    "            'Training History', \n",
    "            'Distribuție Reziduuri', \n",
    "            'Time Series Comparison'\n",
    "        ],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # 1a. Scatter plot predicții vs realitate\n",
    "    sample_size = min(2000, len(y_test_orig))\n",
    "    indices = np.random.choice(len(y_test_orig), sample_size, replace=False)\n",
    "    y_test_sample = y_test_orig[indices]\n",
    "    y_pred_sample = y_pred_orig[indices]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_test_sample, \n",
    "            y=y_pred_sample, \n",
    "            mode='markers', \n",
    "            name=f'Predicții (R²={metrics[\"r2\"]:.4f})',\n",
    "            marker=dict(size=4, opacity=0.6, color='blue'),\n",
    "            hovertemplate='Real: %{x:.6f}<br>Pred: %{y:.6f}<extra></extra>'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Linia perfectă\n",
    "    min_val, max_val = min(y_test_sample.min(), y_pred_sample.min()), max(y_test_sample.max(), y_pred_sample.max())\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[min_val, max_val], \n",
    "            y=[min_val, max_val],\n",
    "            mode='lines', \n",
    "            name='Perfect Fit', \n",
    "            line=dict(dash='dash', color='red')\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 1b. Training history\n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=history.history['loss'], name='Train Loss', line=dict(color='blue')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=history.history['val_loss'], name='Val Loss', line=dict(color='red')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 1c. Histogramă reziduuri\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=residuals[indices], \n",
    "            name='Reziduuri', \n",
    "            nbinsx=50,\n",
    "            marker=dict(color='green', opacity=0.7)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 1d. Time series comparison (primele 500 puncte)\n",
    "    time_sample = min(500, len(y_test_orig))\n",
    "    x_time = np.arange(time_sample)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x_time, y=y_test_orig[:time_sample], name='Original', \n",
    "                  line=dict(color='blue', width=2)),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x_time, y=y_pred_orig[:time_sample], name='Predicție', \n",
    "                  line=dict(color='red', width=2, dash='dot')),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800, \n",
    "        title_text=f\"Analiză Completă Model LSTM - MAE: {metrics['mae']:.6f}, R²: {metrics['r2']:.4f}\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Labels pentru axe\n",
    "    fig.update_xaxes(title_text=\"Valori Reale\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Predicții\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Epocă\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Reziduuri\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Frecvență\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Timp\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Energie\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # 2. Plot separat pentru analiza FFT a predicțiilor\n",
    "    create_fft_analysis_plot(y_test_orig, y_pred_orig)\n",
    "\n",
    "def create_fft_analysis_plot(y_test_orig, y_pred_orig):\n",
    "    \"\"\"Analiză FFT a predicțiilor vs realitate\"\"\"\n",
    "    \n",
    "    # FFT pentru o subsecțiune reprezentativă\n",
    "    sample_size = min(2048, len(y_test_orig))  # Putere de 2 pentru FFT eficient\n",
    "    \n",
    "    y_test_sample = y_test_orig[:sample_size]\n",
    "    y_pred_sample = y_pred_orig[:sample_size]\n",
    "    \n",
    "    # Calculare FFT\n",
    "    fft_test = np.abs(fft(y_test_sample))[:sample_size//2]\n",
    "    fft_pred = np.abs(fft(y_pred_sample))[:sample_size//2]\n",
    "    freqs = fftfreq(sample_size)[:sample_size//2]\n",
    "    \n",
    "    # Plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=freqs, \n",
    "            y=fft_test, \n",
    "            name='FFT Original', \n",
    "            line=dict(color='blue', width=2)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=freqs, \n",
    "            y=fft_pred, \n",
    "            name='FFT Predicții', \n",
    "            line=dict(color='red', width=2, dash='dash')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Evidențiază frecvența dominantă cunoscută (0.02 Hz)\n",
    "    fig.add_vline(\n",
    "        x=0.02, \n",
    "        line_dash=\"dot\", \n",
    "        line_color=\"green\",\n",
    "        annotation_text=\"Freq dominantă (0.02 Hz)\"\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Comparație FFT: Original vs Predicții\",\n",
    "        xaxis_title=\"Frecvența (Hz)\",\n",
    "        yaxis_title=\"Amplitudine\",\n",
    "        height=500,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Zoom pe regiunea de interes (0-0.1 Hz)\n",
    "    fig.update_xaxes(range=[0, 0.1])\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Rulare vizualizări\n",
    "create_comprehensive_plots(results, history)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 11. Predicții pe Date Noi și Validare\n",
    "\n",
    "# %%\n",
    "def make_future_predictions(model, last_sequence, scalers, n_future_steps=100):\n",
    "    \"\"\"\n",
    "    Creează predicții pentru viitor folosind ultimele date\n",
    "    \"\"\"\n",
    "    print(f\"🔮 Predicții pentru următorii {n_future_steps} pași...\")\n",
    "    \n",
    "    # Folosește ultima secvență din datele de test\n",
    "    current_sequence = last_sequence.copy()\n",
    "    future_predictions = []\n",
    "    \n",
    "    # Predicție pas cu pas\n",
    "    for step in range(n_future_steps):\n",
    "        # Predicția pentru următorii OUT_STEPS\n",
    "        pred = model.predict(current_sequence.reshape(1, current_sequence.shape[0], current_sequence.shape[1]), verbose=0)\n",
    "        \n",
    "        # Ia doar primul pas prezis\n",
    "        next_energy = pred[0, 0]\n",
    "        future_predictions.append(next_energy)\n",
    "        \n",
    "        # Actualizează secvența (remove primul element, add predicția)\n",
    "        # Pentru simplitate, păstrăm doar energia (coloana 0) și aproximăm restul features\n",
    "        new_row = current_sequence[-1].copy()\n",
    "        new_row[0] = next_energy  # Update energia\n",
    "        \n",
    "        # Update secvența\n",
    "        current_sequence = np.vstack([current_sequence[1:], new_row])\n",
    "    \n",
    "    # Scalare inversă pentru unități originale\n",
    "    future_predictions = np.array(future_predictions).reshape(-1, 1)\n",
    "    future_predictions_original = scalers['energy_scaler'].inverse_transform(future_predictions).flatten()\n",
    "    \n",
    "    return future_predictions_original\n",
    "\n",
    "# Creează predicții pentru viitor\n",
    "last_test_sequence = X_test[-1]  # Ultima secvență din test\n",
    "future_pred = make_future_predictions(model, last_test_sequence, scalers, n_future_steps=100)\n",
    "\n",
    "# Vizualizează predicțiile viitoare\n",
    "fig = go.Figure()\n",
    "\n",
    "# Ultimele valori cunoscute\n",
    "known_values = scalers['energy_scaler'].inverse_transform(\n",
    "    y_test[-10:].flatten().reshape(-1, 1)\n",
    ").flatten()\n",
    "\n",
    "x_known = np.arange(-len(known_values), 0)\n",
    "x_future = np.arange(0, len(future_pred))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_known, \n",
    "        y=known_values, \n",
    "        name='Valori Cunoscute',\n",
    "        line=dict(color='blue', width=3),\n",
    "        mode='lines+markers'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_future, \n",
    "        y=future_pred, \n",
    "        name='Predicții Viitoare',\n",
    "        line=dict(color='red', width=2, dash='dash'),\n",
    "        mode='lines+markers'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_vline(x=0, line_dash=\"dot\", line_color=\"black\", annotation_text=\"Prezent\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Predicții pentru Viitor - Energie Cinetică\",\n",
    "    xaxis_title=\"Pași de Timp\",\n",
    "    yaxis_title=\"Energie Totală\",\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"📊 Statistici predicții viitoare:\")\n",
    "print(f\"   ├── Min: {future_pred.min():.6f}\")\n",
    "print(f\"   ├── Max: {future_pred.max():.6f}\")\n",
    "print(f\"   ├── Mean: {future_pred.mean():.6f}\")\n",
    "print(f\"   └── Std: {future_pred.std():.6f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 12. Analiza Importanței Features\n",
    "\n",
    "# %%\n",
    "def analyze_feature_importance_approximation(model, X_test, y_test, feature_names, n_samples=100):\n",
    "    \"\"\"\n",
    "    Analiză aproximativă a importanței features prin permutare\n",
    "    \"\"\"\n",
    "    print(\"🔍 Analiză importanță features (aproximativă)...\")\n",
    "    \n",
    "    # Baseline performance\n",
    "    baseline_pred = model.predict(X_test[:n_samples], verbose=0)\n",
    "    baseline_mse = mean_squared_error(y_test[:n_samples].flatten(), baseline_pred.flatten())\n",
    "    \n",
    "    feature_importance = {}\n",
    "    \n",
    "    # Pentru fiecare feature\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        print(f\"   Testing feature: {feature_name}\")\n",
    "        \n",
    "        # Creează o copie și amestecă feature-ul\n",
    "        X_permuted = X_test[:n_samples].copy()\n",
    "        X_permuted[:, :, i] = np.random.permutation(X_permuted[:, :, i].flatten()).reshape(X_permuted[:, :, i].shape)\n",
    "        \n",
    "        # Calculează performanța cu feature-ul amestecat\n",
    "        permuted_pred = model.predict(X_permuted, verbose=0)\n",
    "        permuted_mse = mean_squared_error(y_test[:n_samples].flatten(), permuted_pred.flatten())\n",
    "        \n",
    "        # Importanța = cât de mult se degradează performanța\n",
    "        importance = permuted_mse - baseline_mse\n",
    "        feature_importance[feature_name] = importance\n",
    "    \n",
    "    # Sortează și afișează\n",
    "    sorted_importance = dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    print(f\"\\n🏆 Top 10 features importante:\")\n",
    "    for i, (feature, importance) in enumerate(list(sorted_importance.items())[:10]):\n",
    "        print(f\"   {i+1:2d}. {feature:20s}: {importance:.8f}\")\n",
    "    \n",
    "    # Plot importanța\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    features = list(sorted_importance.keys())[:15]  # Top 15\n",
    "    importances = [sorted_importance[f] for f in features]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=importances,\n",
    "            y=features,\n",
    "            orientation='h',\n",
    "            marker=dict(color=importances, colorscale='Viridis')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Importanța Features (Top 15)\",\n",
    "        xaxis_title=\"Degradarea MSE\",\n",
    "        yaxis_title=\"Features\",\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return sorted_importance\n",
    "\n",
    "# Analiză importanță features\n",
    "feature_importance = analyze_feature_importance_approximation(\n",
    "    model, X_test, y_test, feature_names, n_samples=200\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 13. Salvarea Modelului și Configurației\n",
    "\n",
    "# %%\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model_and_config(model, scalers, feature_names, results, config_info):\n",
    "    \"\"\"Salvează modelul și toate configurațiile\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_name = f\"energy_lstm_model_{timestamp}\"\n",
    "    \n",
    "    print(f\"💾 Salvare model și configurație cu timestamp: {timestamp}\")\n",
    "    \n",
    "    # 1. Salvare model TensorFlow\n",
    "    model.save(f\"{base_name}.h5\")\n",
    "    print(f\"✅ Model salvat: {base_name}.h5\")\n",
    "    \n",
    "    # 2. Salvare scalers\n",
    "    with open(f\"{base_name}_scalers.pkl\", 'wb') as f:\n",
    "        pickle.dump(scalers, f)\n",
    "    print(f\"✅ Scalers salvați: {base_name}_scalers.pkl\")\n",
    "    \n",
    "    # 3. Salvare configurație și rezultate\n",
    "    config = {\n",
    "        'model_info': {\n",
    "            'sequence_length': SEQUENCE_LENGTH,\n",
    "            'output_steps': OUT_STEPS,\n",
    "            'overlap_ratio': OVERLAP_RATIO,\n",
    "            'total_features': len(feature_names),\n",
    "            'training_samples': len(X_train),\n",
    "            'test_samples': len(X_test)\n",
    "        },\n",
    "        'feature_names': feature_names,\n",
    "        'performance_metrics': results['metrics'],\n",
    "        'training_config': {\n",
    "            'epochs_completed': len(history.history['loss']),\n",
    "            'final_train_loss': float(history.history['loss'][-1]),\n",
    "            'final_val_loss': float(history.history['val_loss'][-1]),\n",
    "            'best_val_loss': float(min(history.history['val_loss']))\n",
    "        },\n",
    "        'data_info': config_info,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    with open(f\"{base_name}_config.json\", 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(f\"✅ Configurație salvată: {base_name}_config.json\")\n",
    "    \n",
    "    return base_name\n",
    "\n",
    "# Informații despre date pentru salvare\n",
    "data_config = {\n",
    "    'source_file': my_file,\n",
    "    'total_data_points': len(df_data),\n",
    "    'energy_range': [float(df_data['E_tot'].min()), float(df_data['E_tot'].max())],\n",
    "    'temperature_range': [float(df_data['T'].min()), float(df_data['T'].max())],\n",
    "    'dominant_frequency_hz': float(dominant_freqs[0]) if len(dominant_freqs) > 0 else None,\n",
    "    'dominant_period_steps': float(1/dominant_freqs[0]) if len(dominant_freqs) > 0 and dominant_freqs[0] != 0 else None\n",
    "}\n",
    "\n",
    "# Salvare completă\n",
    "saved_model_name = save_model_and_config(model, scalers, feature_names, results, data_config)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 14. Funcție pentru Încărcarea și Utilizarea Modelului Salvat\n",
    "\n",
    "# %%\n",
    "def load_trained_model(base_name):\n",
    "    \"\"\"Încarcă un model antrenat anterior\"\"\"\n",
    "    print(f\"📂 Încărcare model: {base_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Încărcare model\n",
    "        model = tf.keras.models.load_model(f\"{base_name}.h5\")\n",
    "        print(f\"✅ Model încărcat\")\n",
    "        \n",
    "        # Încărcare scalers\n",
    "        with open(f\"{base_name}_scalers.pkl\", 'rb') as f:\n",
    "            scalers = pickle.load(f)\n",
    "        print(f\"✅ Scalers încărcați\")\n",
    "        \n",
    "        # Încărcare configurație\n",
    "        with open(f\"{base_name}_config.json\", 'r') as f:\n",
    "            config = json.load(f)\n",
    "        print(f\"✅ Configurație încărcată\")\n",
    "        \n",
    "        print(f\"📊 Model info:\")\n",
    "        print(f\"   ├── Features: {config['model_info']['total_features']}\")\n",
    "        print(f\"   ├── Sequence length: {config['model_info']['sequence_length']}\")\n",
    "        print(f\"   ├── Output steps: {config['model_info']['output_steps']}\")\n",
    "        print(f\"   ├── Performance R²: {config['performance_metrics']['r2']:.4f}\")\n",
    "        print(f\"   └── Performance MAE: {config['performance_metrics']['mae']:.6f}\")\n",
    "        \n",
    "        return model, scalers, config\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Eroare la încărcare: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Exemplu de utilizare (decomentează pentru a testa)\n",
    "# loaded_model, loaded_scalers, loaded_config = load_trained_model(saved_model_name)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 15. Funcție pentru Predicții pe Date Noi\n",
    "\n",
    "# %%\n",
    "def predict_energy_sequence(model, scalers, feature_names, new_data, sequence_length):\n",
    "    \"\"\"\n",
    "    Funcție pentru predicții pe date complet noi\n",
    "    \n",
    "    Args:\n",
    "        model: Modelul antrenat\n",
    "        scalers: Scalers pentru normalizare\n",
    "        feature_names: Lista cu numele features\n",
    "        new_data: DataFrame cu date noi (trebuie să aibă coloanele: E_tot, T, P)\n",
    "        sequence_length: Lungimea secvenței de input\n",
    "    \"\"\"\n",
    "    print(f\"🔮 Predicții pe {len(new_data)} puncte noi...\")\n",
    "    \n",
    "    try:\n",
    "        # Verifică că datele au coloanele necesare\n",
    "        required_cols = ['E_tot', 'T', 'P']\n",
    "        if not all(col in new_data.columns for col in required_cols):\n",
    "            raise ValueError(f\"Datele trebuie să conțină coloanele: {required_cols}\")\n",
    "        \n",
    "        # Recreate features similar cu training\n",
    "        energy_scaled = scalers['energy_scaler'].transform(new_data[['E_tot']]).flatten()\n",
    "        temp_scaled = scalers['temp_scaler'].transform(new_data[['T']]).flatten()\n",
    "        pressure_scaled = scalers['pressure_scaler'].transform(new_data[['P']]).flatten()\n",
    "        \n",
    "        # Features derivate\n",
    "        energy_diff = np.gradient(energy_scaled)\n",
    "        energy_diff2 = np.gradient(energy_diff)\n",
    "        \n",
    "        energy_ma5 = pd.Series(energy_scaled).rolling(window=5, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "        energy_ma10 = pd.Series(energy_scaled).rolling(window=10, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "        energy_ma20 = pd.Series(energy_scaled).rolling(window=20, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "        \n",
    "        energy_vol5 = pd.Series(energy_scaled).rolling(window=5, center=True).std().fillna(0)\n",
    "        energy_vol10 = pd.Series(energy_scaled).rolling(window=10, center=True).std().fillna(0)\n",
    "        \n",
    "        temp_diff = np.gradient(temp_scaled)\n",
    "        temp_ma10 = pd.Series(temp_scaled).rolling(window=10, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "        \n",
    "        # Features Fourier (aproximare - ar trebui să fie calculate similar cu training)\n",
    "        fourier_features, _ = create_fourier_features(energy_scaled, dominant_freqs, top_n=3)\n",
    "        \n",
    "        # Combine toate features\n",
    "        all_features = np.column_stack([\n",
    "            energy_scaled, temp_scaled, pressure_scaled,\n",
    "            energy_diff, energy_diff2,\n",
    "            energy_ma5, energy_ma10, energy_ma20,\n",
    "            energy_vol5, energy_vol10,\n",
    "            temp_diff, temp_ma10,\n",
    "            fourier_features\n",
    "        ])\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # Predicții pentru fiecare secvență posibilă\n",
    "        for i in range(len(all_features) - sequence_length + 1):\n",
    "            seq = all_features[i:i + sequence_length].reshape(1, sequence_length, -1)\n",
    "            pred = model.predict(seq, verbose=0)\n",
    "            predictions.append(pred[0])\n",
    "        \n",
    "        # Scalare inversă\n",
    "        predictions = np.array(predictions)\n",
    "        predictions_original = scalers['energy_scaler'].inverse_transform(\n",
    "            predictions.reshape(-1, 1)\n",
    "        ).reshape(predictions.shape)\n",
    "        \n",
    "        print(f\"✅ {len(predictions)} predicții generate\")\n",
    "        \n",
    "        return predictions_original\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Eroare la predicție: {e}\")\n",
    "        return None\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 16. Rezumat Final și Recomandări\n",
    "\n",
    "# %%\n",
    "print(\"🎯 REZUMAT FINAL - ANALIZA TIME SERIES ENERGIE CINETICĂ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n📊 PERFORMANȚA MODELULUI:\")\n",
    "print(f\"   ├── R² Score: {results['metrics']['r2']:.6f}\")\n",
    "print(f\"   ├── Mean Absolute Error: {results['metrics']['mae']:.8f}\")\n",
    "print(f\"   ├── Root Mean Square Error: {results['metrics']['rmse']:.8f}\")\n",
    "print(f\"   └── Mean Absolute Percentage Error: {results['metrics']['mape']:.4f}%\")\n",
    "\n",
    "print(f\"\\n🔧 CONFIGURAȚIA OPTIMALĂ:\")\n",
    "print(f\"   ├── Sequence Length: {SEQUENCE_LENGTH} pași (2× perioada principală)\")\n",
    "print(f\"   ├── Output Steps: {OUT_STEPS} pași (0.5× perioada)\")\n",
    "print(f\"   ├── Total Features: {len(feature_names)}\")\n",
    "print(f\"   ├── Overlap Ratio: {OVERLAP_RATIO} ({int(OVERLAP_RATIO*100)}%)\")\n",
    "print(f\"   └── Training Samples: {len(X_train):,}\")\n",
    "\n",
    "print(f\"\\n🌊 ANALIZA FOURIER:\")\n",
    "if len(dominant_freqs) > 0:\n",
    "    print(f\"   ├── Frecvența dominantă: {dominant_freqs[0]:.6f} Hz\")\n",
    "    print(f\"   ├── Perioada principală: {1/dominant_freqs[0]:.2f} pași\")\n",
    "    print(f\"   └── Features Fourier generate: {len([f for f in feature_names if 'sin_' in f or 'cos_' in f])}\")\n",
    "\n",
    "print(f\"\\n💡 RECOMANDĂRI PENTRU OPTIMIZARE ULTERIOARĂ:\")\n",
    "print(f\"   ├── Experimentează cu arhitecturi Transformer pentru capturarea dependințelor\")\n",
    "print(f\"   ├── Încearcă ensemble methods cu multiple modele LSTM\")\n",
    "print(f\"   ├── Consideră attention mechanisms pentru features importante\")\n",
    "print(f\"   ├── Testează data augmentation prin rotații de fază\")\n",
    "print(f\"   └── Explorează autoencoder pentru detecția anomaliilor\")\n",
    "\n",
    "print(f\"\\n📁 FIȘIERE SALVATE:\")\n",
    "print(f\"   ├── Model: {saved_model_name}.h5\")\n",
    "print(f\"   ├── Scalers: {saved_model_name}_scalers.pkl\")\n",
    "print(f\"   └── Config: {saved_model_name}_config.json\")\n",
    "\n",
    "print(f\"\\n🚀 URMĂTORII PAȘI:\")\n",
    "print(f\"   1. Testează modelul pe date complet noi\")\n",
    "print(f\"   2. Implementează monitoring în timp real\")\n",
    "print(f\"   3. Optimizează hiperparametrii cu Optuna/Hyperopt\")\n",
    "print(f\"   4. Dezvoltă API pentru predicții în producție\")\n",
    "\n",
    "print(\"\\n✅ ANALIZA COMPLETĂ FINALIZATĂ!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
