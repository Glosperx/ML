{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95335a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # AnalizÄƒ Time Series - Energie CineticÄƒ Pentilfuran cu FFT\n",
    "# \n",
    "# Acest notebook analizeazÄƒ datele de energie cineticÄƒ pentru molecula de pentilfuran Ã®ntr-un cÃ¢mp electric.\n",
    "# Folosim transformata Fourier pentru a exploata natura sinusoidalÄƒ a datelor.\n",
    "# \n",
    "# **Parametri cunoscuÈ›i:**\n",
    "# - FrecvenÈ›a dominantÄƒ: 0.020000 Hz\n",
    "# - Perioada principalÄƒ: 50.00 paÈ™i de timp\n",
    "# - Window size recomandat: 100 paÈ™i\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Import Libraries È™i Configurare\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq, ifft\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurare pentru reproducibilitate\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"ğŸ“¦ Libraries importate cu succes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684946fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 2. ÃncÄƒrcarea È™i Preprocesarea Datelor\n",
    "\n",
    "# %%\n",
    "# ÃncÄƒrcare date\n",
    "my_file = \"./pentilfuran.MDE\"\n",
    "\n",
    "df = pd.read_csv(\n",
    "    my_file,\n",
    "    sep=r\"\\s+\",\n",
    "    comment='#',\n",
    "    names=[\"Step\", \"T\", \"E_KS\", \"E_tot\", \"Vol\", \"P\"]\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Dimensiune date originale: {df.shape}\")\n",
    "print(f\"ğŸ“‹ Coloane disponibile: {df.columns.tolist()}\")\n",
    "print(f\"ğŸ”¢ NumÄƒrul de steps unici: {df['Step'].nunique()}\")\n",
    "\n",
    "# %%\n",
    "# Selectare liniile 1:901 pentru fiecare Step (optimizat)\n",
    "df_data = (\n",
    "    df.groupby(\"Step\", group_keys=False)\n",
    "    .apply(lambda g: g.iloc[1:901])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dimensiunea dupÄƒ filtrare: {len(df_data)} rÃ¢nduri\")\n",
    "print(f\"ğŸ“ˆ Range energie totalÄƒ: [{df_data['E_tot'].min():.6f}, {df_data['E_tot'].max():.6f}]\")\n",
    "print(f\"ğŸŒ¡ï¸ Range temperaturÄƒ: [{df_data['T'].min():.4f}, {df_data['T'].max():.4f}]\")\n",
    "\n",
    "# Verificare pentru valori lipsÄƒ\n",
    "print(f\"\\nğŸ” Valori lipsÄƒ per coloanÄƒ:\")\n",
    "print(df_data.isnull().sum())\n",
    "\n",
    "# %% [markdown]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27cde6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3. Analiza PeriodicitÄƒÈ›ii cu FFT\n",
    "\n",
    "# %%\n",
    "def analyze_periodicity(signal_data, sampling_rate=1.0, plot=True):\n",
    "    \"\"\"AnalizÄƒ avansatÄƒ de periodicitate cu FFT\"\"\"\n",
    "    print(\"ğŸ” AnalizÄƒ periodicitate cu FFT...\")\n",
    "    \n",
    "    # Remove trend pentru FFT mai precisÄƒ\n",
    "    detrended_signal = signal.detrend(signal_data)\n",
    "    \n",
    "    # FFT\n",
    "    fft_values = fft(detrended_signal)\n",
    "    frequencies = fftfreq(len(signal_data), d=1/sampling_rate)\n",
    "    \n",
    "    # Power spectrum (doar frecvenÈ›ele pozitive, fÄƒrÄƒ DC)\n",
    "    power_spectrum = np.abs(fft_values[1:len(signal_data)//2])\n",
    "    freqs_positive = frequencies[1:len(signal_data)//2]\n",
    "    \n",
    "    # Top 10 frecvenÈ›e dominante\n",
    "    dominant_indices = np.argsort(power_spectrum)[-10:][::-1]\n",
    "    dominant_freqs = freqs_positive[dominant_indices]\n",
    "    dominant_powers = power_spectrum[dominant_indices]\n",
    "    \n",
    "    print(f\"ğŸ¯ Top 10 frecvenÈ›e dominante:\")\n",
    "    for i, (freq, power) in enumerate(zip(dominant_freqs, dominant_powers)):\n",
    "        period = 1/freq if freq != 0 else np.inf\n",
    "        print(f\"   {i+1:2d}. Freq: {freq:.6f} Hz, PerioadÄƒ: {period:8.2f} paÈ™i, Putere: {power:.2e}\")\n",
    "    \n",
    "    if plot:\n",
    "        # Plot FFT\n",
    "        fig = sp.make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            subplot_titles=['Semnal Original vs Detrended', 'Power Spectrum (FFT)']\n",
    "        )\n",
    "        \n",
    "        # Semnal original vs detrended (primele 1000 puncte pentru vizibilitate)\n",
    "        sample_size = min(1000, len(signal_data))\n",
    "        x_axis = np.arange(sample_size)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=x_axis, y=signal_data[:sample_size], \n",
    "                      name='Original', line=dict(color='blue')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=x_axis, y=detrended_signal[:sample_size], \n",
    "                      name='Detrended', line=dict(color='red')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Power spectrum (zoom pe primele 50 frecvenÈ›e pentru claritate)\n",
    "        freq_limit = min(50, len(freqs_positive))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=freqs_positive[:freq_limit], y=power_spectrum[:freq_limit],\n",
    "                      mode='lines+markers', name='Power Spectrum'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # EvidenÈ›iazÄƒ frecvenÈ›ele dominante\n",
    "        for i, (freq, power) in enumerate(zip(dominant_freqs[:5], dominant_powers[:5])):\n",
    "            if freq <= freqs_positive[freq_limit-1]:  # Doar dacÄƒ e Ã®n range-ul vizualizat\n",
    "                fig.add_vline(x=freq, line_dash=\"dash\", \n",
    "                             annotation_text=f\"f{i+1}={freq:.4f}Hz\", \n",
    "                             row=2, col=1)\n",
    "        \n",
    "        fig.update_layout(height=800, title_text=\"AnalizÄƒ FFT - Energie TotalÄƒ\")\n",
    "        fig.update_xaxes(title_text=\"Timp (paÈ™i)\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"FrecvenÈ›a (Hz)\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Energie\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Amplitudine\", row=2, col=1)\n",
    "        fig.show()\n",
    "    \n",
    "    return dominant_freqs, power_spectrum, frequencies, detrended_signal\n",
    "\n",
    "# Rulare analizÄƒ FFT pe energia totalÄƒ\n",
    "energy_data = df_data['E_tot'].values\n",
    "dominant_freqs, power_spectrum, frequencies, detrended_energy = analyze_periodicity(energy_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe790ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Crearea Features Fourier\n",
    "\n",
    "# %%\n",
    "def create_fourier_features(data, dominant_freqs, top_n=5):\n",
    "    \"\"\"CreeazÄƒ features bazate pe componentele Fourier dominante\"\"\"\n",
    "    print(f\"ğŸŒŠ Creare {top_n} features Fourier...\")\n",
    "    \n",
    "    fourier_features = []\n",
    "    feature_names = []\n",
    "    \n",
    "    t = np.arange(len(data))\n",
    "    \n",
    "    for i, freq in enumerate(dominant_freqs[:top_n]):\n",
    "        # Sin È™i Cos pentru fiecare frecvenÈ›Äƒ dominantÄƒ\n",
    "        sin_component = np.sin(2 * np.pi * freq * t)\n",
    "        cos_component = np.cos(2 * np.pi * freq * t)\n",
    "        \n",
    "        fourier_features.extend([sin_component, cos_component])\n",
    "        feature_names.extend([f'sin_f{i+1}_{freq:.4f}Hz', f'cos_f{i+1}_{freq:.4f}Hz'])\n",
    "    \n",
    "    print(f\"âœ… Features Fourier create: {feature_names}\")\n",
    "    return np.array(fourier_features).T, feature_names\n",
    "\n",
    "# Crearea features Fourier\n",
    "fourier_features, fourier_names = create_fourier_features(energy_data, dominant_freqs, top_n=3)\n",
    "print(f\"ğŸ“ Shape features Fourier: {fourier_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0473b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Feature Engineering Complet\n",
    "\n",
    "# %%\n",
    "def create_comprehensive_features(df_data, fourier_features, fourier_names):\n",
    "    \"\"\"CreeazÄƒ un set complet de features pentru training\"\"\"\n",
    "    print(\"ğŸ”§ Creare features comprehensive...\")\n",
    "    \n",
    "    # Scalere pentru diferite tipuri de date\n",
    "    energy_scaler = StandardScaler()\n",
    "    temp_scaler = StandardScaler()\n",
    "    pressure_scaler = StandardScaler()\n",
    "    \n",
    "    # Features de bazÄƒ scalate\n",
    "    energy_scaled = energy_scaler.fit_transform(df_data[['E_tot']]).flatten()\n",
    "    temp_scaled = temp_scaler.fit_transform(df_data[['T']]).flatten()\n",
    "    pressure_scaled = pressure_scaler.fit_transform(df_data[['P']]).flatten()\n",
    "    \n",
    "    # Features derivate pentru energie\n",
    "    energy_diff = np.gradient(energy_scaled)\n",
    "    energy_diff2 = np.gradient(energy_diff)  # AcceleraÈ›ie\n",
    "    \n",
    "    # Moving averages (pentru capturarea trend-urilor)\n",
    "    energy_ma5 = pd.Series(energy_scaled).rolling(window=5, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "    energy_ma10 = pd.Series(energy_scaled).rolling(window=10, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "    energy_ma20 = pd.Series(energy_scaled).rolling(window=20, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "    \n",
    "    # Volatilitate (rolling std)\n",
    "    energy_vol5 = pd.Series(energy_scaled).rolling(window=5, center=True).std().fillna(0)\n",
    "    energy_vol10 = pd.Series(energy_scaled).rolling(window=10, center=True).std().fillna(0)\n",
    "    \n",
    "    # Features pentru temperaturÄƒ\n",
    "    temp_diff = np.gradient(temp_scaled)\n",
    "    temp_ma10 = pd.Series(temp_scaled).rolling(window=10, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "    \n",
    "    # Combinare toate features\n",
    "    all_features = np.column_stack([\n",
    "        energy_scaled,              # Target principal\n",
    "        temp_scaled,                # TemperaturÄƒ\n",
    "        pressure_scaled,            # Presiune\n",
    "        energy_diff,                # Viteza energiei\n",
    "        energy_diff2,               # AcceleraÈ›ia energiei\n",
    "        energy_ma5,                 # Trend pe termen scurt\n",
    "        energy_ma10,                # Trend pe termen mediu\n",
    "        energy_ma20,                # Trend pe termen lung\n",
    "        energy_vol5,                # Volatilitate scurtÄƒ\n",
    "        energy_vol10,               # Volatilitate medie\n",
    "        temp_diff,                  # Rata schimbÄƒrii temperaturii\n",
    "        temp_ma10,                  # Trend temperaturÄƒ\n",
    "        fourier_features           # Features Fourier\n",
    "    ])\n",
    "    \n",
    "    # Namen features\n",
    "    feature_names = [\n",
    "        'Energy_scaled', 'Temp_scaled', 'Pressure_scaled',\n",
    "        'Energy_velocity', 'Energy_acceleration',\n",
    "        'Energy_MA5', 'Energy_MA10', 'Energy_MA20',\n",
    "        'Energy_Vol5', 'Energy_Vol10',\n",
    "        'Temp_velocity', 'Temp_MA10'\n",
    "    ] + fourier_names\n",
    "    \n",
    "    print(f\"âœ… Total features: {all_features.shape[1]}\")\n",
    "    print(f\"ğŸ“‹ Feature names: {feature_names}\")\n",
    "    \n",
    "    return all_features, feature_names, {\n",
    "        'energy_scaler': energy_scaler,\n",
    "        'temp_scaler': temp_scaler,\n",
    "        'pressure_scaler': pressure_scaler\n",
    "    }\n",
    "\n",
    "# Creare features complete\n",
    "all_features, feature_names, scalers = create_comprehensive_features(df_data, fourier_features, fourier_names)\n",
    "\n",
    "# Verificare pentru NaN sau Inf\n",
    "print(f\"\\nğŸ” Verificare calitatea features:\")\n",
    "print(f\"NaN values: {np.isnan(all_features).sum()}\")\n",
    "print(f\"Inf values: {np.isinf(all_features).sum()}\")\n",
    "print(f\"Feature range: [{all_features.min():.4f}, {all_features.max():.4f}]\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Crearea SecvenÈ›elor pentru LSTM\n",
    "\n",
    "# %%\n",
    "def create_sequences_optimized(features, energy_target, sequence_length, out_steps, overlap_ratio=0.8):\n",
    "    \"\"\"\n",
    "    CreeazÄƒ secvenÈ›e optimizate cu overlap pentru mai multe date de training\n",
    "    \n",
    "    Args:\n",
    "        features: Array cu toate features (include target la coloana 0)\n",
    "        energy_target: Target-ul pentru predicÈ›ie (energia scalatÄƒ)\n",
    "        sequence_length: Lungimea secvenÈ›ei input\n",
    "        out_steps: NumÄƒrul de paÈ™i de prezis\n",
    "        overlap_ratio: Raportul de overlap Ã®ntre secvenÈ›e (0.8 = 80% overlap)\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”§ Creare secvenÈ›e cu parametri:\")\n",
    "    print(f\"   - Sequence length: {sequence_length}\")\n",
    "    print(f\"   - Output steps: {out_steps}\")\n",
    "    print(f\"   - Overlap ratio: {overlap_ratio}\")\n",
    "    \n",
    "    sequences, targets = [], []\n",
    "    \n",
    "    # CalculeazÄƒ pas-ul bazat pe overlap\n",
    "    step_size = max(1, int(sequence_length * (1 - overlap_ratio)))\n",
    "    \n",
    "    # GenereazÄƒ secvenÈ›e cu overlap\n",
    "    for i in range(0, len(features) - sequence_length - out_steps + 1, step_size):\n",
    "        # Input sequence (toate features)\n",
    "        seq = features[i:i + sequence_length]\n",
    "        \n",
    "        # Target sequence (doar energia)\n",
    "        target = energy_target[i + sequence_length:i + sequence_length + out_steps]\n",
    "        \n",
    "        if len(target) == out_steps:  # VerificÄƒ cÄƒ target-ul e complet\n",
    "            sequences.append(seq)\n",
    "            targets.append(target)\n",
    "    \n",
    "    sequences = np.array(sequences)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    print(f\"âœ… SecvenÈ›e create:\")\n",
    "    print(f\"   - Input shape: {sequences.shape}\")\n",
    "    print(f\"   - Target shape: {targets.shape}\")\n",
    "    print(f\"   - Total samples: {len(sequences)}\")\n",
    "    \n",
    "    return sequences, targets\n",
    "\n",
    "# Parametrii optimizaÈ›i pentru perioada de 50 paÈ™i\n",
    "SEQUENCE_LENGTH = 100  # 2x perioada principalÄƒ\n",
    "OUT_STEPS = 25         # 0.5x perioada pentru predicÈ›ii precise\n",
    "OVERLAP_RATIO = 0.7    # 70% overlap pentru mai multe sample-uri\n",
    "\n",
    "# Crearea secvenÈ›elor\n",
    "energy_target = all_features[:, 0]  # Prima coloanÄƒ e energia scalatÄƒ\n",
    "sequences, targets = create_sequences_optimized(\n",
    "    all_features, \n",
    "    energy_target, \n",
    "    SEQUENCE_LENGTH, \n",
    "    OUT_STEPS,\n",
    "    OVERLAP_RATIO\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Construirea Modelului LSTM Avansat\n",
    "\n",
    "# %%\n",
    "def build_advanced_lstm_model(input_shape, out_steps, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    ConstruieÈ™te model LSTM avansat optimizat pentru date periodice\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ—ï¸ Construire model LSTM pentru:\")\n",
    "    print(f\"   - Input shape: {input_shape}\")\n",
    "    print(f\"   - Output steps: {out_steps}\")\n",
    "    print(f\"   - Dropout rate: {dropout_rate}\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        # Layer 1: Bidirectional LSTM pentru capturarea dependinÈ›elor Ã®n ambele direcÈ›ii\n",
    "        Bidirectional(\n",
    "            LSTM(128, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate),\n",
    "            input_shape=input_shape,\n",
    "            name='bidirectional_lstm_1'\n",
    "        ),\n",
    "        BatchNormalization(name='batch_norm_1'),\n",
    "        \n",
    "        # Layer 2: Al doilea LSTM bidirectional\n",
    "        Bidirectional(\n",
    "            LSTM(64, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate),\n",
    "            name='bidirectional_lstm_2'\n",
    "        ),\n",
    "        BatchNormalization(name='batch_norm_2'),\n",
    "        \n",
    "        # Layer 3: LSTM final\n",
    "        LSTM(32, dropout=dropout_rate, recurrent_dropout=dropout_rate, name='lstm_final'),\n",
    "        BatchNormalization(name='batch_norm_3'),\n",
    "        \n",
    "        # Dense layers cu regularizare\n",
    "        Dense(64, activation='relu', name='dense_1'),\n",
    "        Dropout(dropout_rate + 0.1, name='dropout_1'),\n",
    "        \n",
    "        Dense(32, activation='relu', name='dense_2'),\n",
    "        Dropout(dropout_rate, name='dropout_2'),\n",
    "        \n",
    "        Dense(16, activation='relu', name='dense_3'),\n",
    "        Dropout(dropout_rate * 0.5, name='dropout_3'),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(out_steps, activation='linear', name='output')\n",
    "    ])\n",
    "    \n",
    "    # Optimizer cu parametri optimizaÈ›i\n",
    "    optimizer = Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7\n",
    "    )\n",
    "    \n",
    "    # Compilare cu loss function robust\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='huber',  # Mai robust la outliers decÃ¢t MSE\n",
    "        metrics=['mae', 'mse', 'mape']\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Model construit cu succes!\")\n",
    "    print(f\"ğŸ“Š Parametri totali: {model.count_params():,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Construire model\n",
    "model = build_advanced_lstm_model(\n",
    "    input_shape=(sequences.shape[1], sequences.shape[2]),\n",
    "    out_steps=OUT_STEPS,\n",
    "    dropout_rate=0.3\n",
    ")\n",
    "\n",
    "# AfiÈ™are arhitectura modelului\n",
    "model.summary()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. ÃmpÄƒrÈ›irea Datelor È™i Antrenarea\n",
    "\n",
    "# %%\n",
    "# Split date (fÄƒrÄƒ shuffle pentru time series)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sequences, targets, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=False  # Important pentru time series\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š ÃmpÄƒrÈ›irea datelor:\")\n",
    "print(f\"   - Train samples: {X_train.shape[0]}\")\n",
    "print(f\"   - Test samples: {X_test.shape[0]}\")\n",
    "print(f\"   - Features per sample: {X_train.shape[2]}\")\n",
    "print(f\"   - Sequence length: {X_train.shape[1]}\")\n",
    "\n",
    "# Definire callbacks avansate\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        min_delta=1e-6\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1,\n",
    "        cooldown=5\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_energy_lstm_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "        save_weights_only=False\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"ğŸš€ Ãncepe antrenarea...\")\n",
    "\n",
    "# %% \n",
    "# Antrenare model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"âœ… Antrenarea completatÄƒ!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Evaluarea Modelului\n",
    "\n",
    "# %%\n",
    "def evaluate_comprehensive(model, X_test, y_test, scalers, feature_names):\n",
    "    \"\"\"Evaluare comprehensivÄƒ a modelului\"\"\"\n",
    "    print(\"ğŸ“ˆ Evaluare model...\")\n",
    "    \n",
    "    # PredicÈ›ii\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Scalare inversÄƒ pentru metrici Ã®n unitÄƒÈ›i originale\n",
    "    energy_scaler = scalers['energy_scaler']\n",
    "    \n",
    "    # Flatten pentru calcularea metricilor\n",
    "    y_test_flat = y_test.flatten().reshape(-1, 1)\n",
    "    y_pred_flat = y_pred.flatten().reshape(-1, 1)\n",
    "    \n",
    "    # Scalare inversÄƒ\n",
    "    y_test_original = energy_scaler.inverse_transform(y_test_flat).flatten()\n",
    "    y_pred_original = energy_scaler.inverse_transform(y_pred_flat).flatten()\n",
    "    \n",
    "    # Calculare metrici\n",
    "    mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "    mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "    r2 = r2_score(y_test_original, y_pred_original)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # MAPE (Mean Absolute Percentage Error)\n",
    "    mape = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100\n",
    "    \n",
    "    print(f\"ğŸ† PerformanÈ›a modelului:\")\n",
    "    print(f\"   â”œâ”€â”€ MSE: {mse:.8f}\")\n",
    "    print(f\"   â”œâ”€â”€ MAE: {mae:.8f}\")\n",
    "    print(f\"   â”œâ”€â”€ RMSE: {rmse:.8f}\")\n",
    "    print(f\"   â”œâ”€â”€ RÂ²: {r2:.6f}\")\n",
    "    print(f\"   â””â”€â”€ MAPE: {mape:.4f}%\")\n",
    "    \n",
    "    # Analiza reziduurilor\n",
    "    residuals = y_pred_original - y_test_original\n",
    "    print(f\"\\nğŸ“Š Analiza reziduurilor:\")\n",
    "    print(f\"   â”œâ”€â”€ Mean residual: {np.mean(residuals):.8f}\")\n",
    "    print(f\"   â”œâ”€â”€ Std residual: {np.std(residuals):.8f}\")\n",
    "    print(f\"   â”œâ”€â”€ Min residual: {np.min(residuals):.8f}\")\n",
    "    print(f\"   â””â”€â”€ Max residual: {np.max(residuals):.8f}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': y_pred,\n",
    "        'predictions_original': y_pred_original,\n",
    "        'targets_original': y_test_original,\n",
    "        'residuals': residuals,\n",
    "        'metrics': {\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mape': mape\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Evaluare model\n",
    "results = evaluate_comprehensive(model, X_test, y_test, scalers, feature_names)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. VizualizÄƒri Complete\n",
    "\n",
    "# %%\n",
    "def create_comprehensive_plots(results, history):\n",
    "    \"\"\"CreeazÄƒ vizualizÄƒri complete pentru analiza modelului\"\"\"\n",
    "    print(\"ğŸ“Š Creare vizualizÄƒri...\")\n",
    "    \n",
    "    # Extrage rezultatele\n",
    "    y_pred_orig = results['predictions_original']\n",
    "    y_test_orig = results['targets_original']\n",
    "    residuals = results['residuals']\n",
    "    metrics = results['metrics']\n",
    "    \n",
    "    # 1. Plot principal cu 4 subplots\n",
    "    fig = sp.make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[\n",
    "            'PredicÈ›ii vs Realitate', \n",
    "            'Training History', \n",
    "            'DistribuÈ›ie Reziduuri', \n",
    "            'Time Series Comparison'\n",
    "        ],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # 1a. Scatter plot predicÈ›ii vs realitate\n",
    "    sample_size = min(2000, len(y_test_orig))\n",
    "    indices = np.random.choice(len(y_test_orig), sample_size, replace=False)\n",
    "    y_test_sample = y_test_orig[indices]\n",
    "    y_pred_sample = y_pred_orig[indices]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_test_sample, \n",
    "            y=y_pred_sample, \n",
    "            mode='markers', \n",
    "            name=f'PredicÈ›ii (RÂ²={metrics[\"r2\"]:.4f})',\n",
    "            marker=dict(size=4, opacity=0.6, color='blue'),\n",
    "            hovertemplate='Real: %{x:.6f}<br>Pred: %{y:.6f}<extra></extra>'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Linia perfectÄƒ\n",
    "    min_val, max_val = min(y_test_sample.min(), y_pred_sample.min()), max(y_test_sample.max(), y_pred_sample.max())\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[min_val, max_val], \n",
    "            y=[min_val, max_val],\n",
    "            mode='lines', \n",
    "            name='Perfect Fit', \n",
    "            line=dict(dash='dash', color='red')\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 1b. Training history\n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=history.history['loss'], name='Train Loss', line=dict(color='blue')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=history.history['val_loss'], name='Val Loss', line=dict(color='red')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 1c. HistogramÄƒ reziduuri\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=residuals[indices], \n",
    "            name='Reziduuri', \n",
    "            nbinsx=50,\n",
    "            marker=dict(color='green', opacity=0.7)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 1d. Time series comparison (primele 500 puncte)\n",
    "    time_sample = min(500, len(y_test_orig))\n",
    "    x_time = np.arange(time_sample)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x_time, y=y_test_orig[:time_sample], name='Original', \n",
    "                  line=dict(color='blue', width=2)),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x_time, y=y_pred_orig[:time_sample], name='PredicÈ›ie', \n",
    "                  line=dict(color='red', width=2, dash='dot')),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800, \n",
    "        title_text=f\"AnalizÄƒ CompletÄƒ Model LSTM - MAE: {metrics['mae']:.6f}, RÂ²: {metrics['r2']:.4f}\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Labels pentru axe\n",
    "    fig.update_xaxes(title_text=\"Valori Reale\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"PredicÈ›ii\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"EpocÄƒ\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Reziduuri\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"FrecvenÈ›Äƒ\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Timp\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Energie\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # 2. Plot separat pentru analiza FFT a predicÈ›iilor\n",
    "    create_fft_analysis_plot(y_test_orig, y_pred_orig)\n",
    "\n",
    "def create_fft_analysis_plot(y_test_orig, y_pred_orig):\n",
    "    \"\"\"AnalizÄƒ FFT a predicÈ›iilor vs realitate\"\"\"\n",
    "    \n",
    "    # FFT pentru o subsecÈ›iune reprezentativÄƒ\n",
    "    sample_size = min(2048, len(y_test_orig))  # Putere de 2 pentru FFT eficient\n",
    "    \n",
    "    y_test_sample = y_test_orig[:sample_size]\n",
    "    y_pred_sample = y_pred_orig[:sample_size]\n",
    "    \n",
    "    # Calculare FFT\n",
    "    fft_test = np.abs(fft(y_test_sample))[:sample_size//2]\n",
    "    fft_pred = np.abs(fft(y_pred_sample))[:sample_size//2]\n",
    "    freqs = fftfreq(sample_size)[:sample_size//2]\n",
    "    \n",
    "    # Plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=freqs, \n",
    "            y=fft_test, \n",
    "            name='FFT Original', \n",
    "            line=dict(color='blue', width=2)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=freqs, \n",
    "            y=fft_pred, \n",
    "            name='FFT PredicÈ›ii', \n",
    "            line=dict(color='red', width=2, dash='dash')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # EvidenÈ›iazÄƒ frecvenÈ›a dominantÄƒ cunoscutÄƒ (0.02 Hz)\n",
    "    fig.add_vline(\n",
    "        x=0.02, \n",
    "        line_dash=\"dot\", \n",
    "        line_color=\"green\",\n",
    "        annotation_text=\"Freq dominantÄƒ (0.02 Hz)\"\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"ComparaÈ›ie FFT: Original vs PredicÈ›ii\",\n",
    "        xaxis_title=\"FrecvenÈ›a (Hz)\",\n",
    "        yaxis_title=\"Amplitudine\",\n",
    "        height=500,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Zoom pe regiunea de interes (0-0.1 Hz)\n",
    "    fig.update_xaxes(range=[0, 0.1])\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Rulare vizualizÄƒri\n",
    "create_comprehensive_plots(results, history)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 11. PredicÈ›ii pe Date Noi È™i Validare\n",
    "\n",
    "# %%\n",
    "def make_future_predictions(model, last_sequence, scalers, n_future_steps=100):\n",
    "    \"\"\"\n",
    "    CreeazÄƒ predicÈ›ii pentru viitor folosind ultimele date\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”® PredicÈ›ii pentru urmÄƒtorii {n_future_steps} paÈ™i...\")\n",
    "    \n",
    "    # FoloseÈ™te ultima secvenÈ›Äƒ din datele de test\n",
    "    current_sequence = last_sequence.copy()\n",
    "    future_predictions = []\n",
    "    \n",
    "    # PredicÈ›ie pas cu pas\n",
    "    for step in range(n_future_steps):\n",
    "        # PredicÈ›ia pentru urmÄƒtorii OUT_STEPS\n",
    "        pred = model.predict(current_sequence.reshape(1, current_sequence.shape[0], current_sequence.shape[1]), verbose=0)\n",
    "        \n",
    "        # Ia doar primul pas prezis\n",
    "        next_energy = pred[0, 0]\n",
    "        future_predictions.append(next_energy)\n",
    "        \n",
    "        # ActualizeazÄƒ secvenÈ›a (remove primul element, add predicÈ›ia)\n",
    "        # Pentru simplitate, pÄƒstrÄƒm doar energia (coloana 0) È™i aproximÄƒm restul features\n",
    "        new_row = current_sequence[-1].copy()\n",
    "        new_row[0] = next_energy  # Update energia\n",
    "        \n",
    "        # Update secvenÈ›a\n",
    "        current_sequence = np.vstack([current_sequence[1:], new_row])\n",
    "    \n",
    "    # Scalare inversÄƒ pentru unitÄƒÈ›i originale\n",
    "    future_predictions = np.array(future_predictions).reshape(-1, 1)\n",
    "    future_predictions_original = scalers['energy_scaler'].inverse_transform(future_predictions).flatten()\n",
    "    \n",
    "    return future_predictions_original\n",
    "\n",
    "# CreeazÄƒ predicÈ›ii pentru viitor\n",
    "last_test_sequence = X_test[-1]  # Ultima secvenÈ›Äƒ din test\n",
    "future_pred = make_future_predictions(model, last_test_sequence, scalers, n_future_steps=100)\n",
    "\n",
    "# VizualizeazÄƒ predicÈ›iile viitoare\n",
    "fig = go.Figure()\n",
    "\n",
    "# Ultimele valori cunoscute\n",
    "known_values = scalers['energy_scaler'].inverse_transform(\n",
    "    y_test[-10:].flatten().reshape(-1, 1)\n",
    ").flatten()\n",
    "\n",
    "x_known = np.arange(-len(known_values), 0)\n",
    "x_future = np.arange(0, len(future_pred))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_known, \n",
    "        y=known_values, \n",
    "        name='Valori Cunoscute',\n",
    "        line=dict(color='blue', width=3),\n",
    "        mode='lines+markers'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_future, \n",
    "        y=future_pred, \n",
    "        name='PredicÈ›ii Viitoare',\n",
    "        line=dict(color='red', width=2, dash='dash'),\n",
    "        mode='lines+markers'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_vline(x=0, line_dash=\"dot\", line_color=\"black\", annotation_text=\"Prezent\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"PredicÈ›ii pentru Viitor - Energie CineticÄƒ\",\n",
    "    xaxis_title=\"PaÈ™i de Timp\",\n",
    "    yaxis_title=\"Energie TotalÄƒ\",\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"ğŸ“Š Statistici predicÈ›ii viitoare:\")\n",
    "print(f\"   â”œâ”€â”€ Min: {future_pred.min():.6f}\")\n",
    "print(f\"   â”œâ”€â”€ Max: {future_pred.max():.6f}\")\n",
    "print(f\"   â”œâ”€â”€ Mean: {future_pred.mean():.6f}\")\n",
    "print(f\"   â””â”€â”€ Std: {future_pred.std():.6f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 12. Analiza ImportanÈ›ei Features\n",
    "\n",
    "# %%\n",
    "def analyze_feature_importance_approximation(model, X_test, y_test, feature_names, n_samples=100):\n",
    "    \"\"\"\n",
    "    AnalizÄƒ aproximativÄƒ a importanÈ›ei features prin permutare\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” AnalizÄƒ importanÈ›Äƒ features (aproximativÄƒ)...\")\n",
    "    \n",
    "    # Baseline performance\n",
    "    baseline_pred = model.predict(X_test[:n_samples], verbose=0)\n",
    "    baseline_mse = mean_squared_error(y_test[:n_samples].flatten(), baseline_pred.flatten())\n",
    "    \n",
    "    feature_importance = {}\n",
    "    \n",
    "    # Pentru fiecare feature\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        print(f\"   Testing feature: {feature_name}\")\n",
    "        \n",
    "        # CreeazÄƒ o copie È™i amestecÄƒ feature-ul\n",
    "        X_permuted = X_test[:n_samples].copy()\n",
    "        X_permuted[:, :, i] = np.random.permutation(X_permuted[:, :, i].flatten()).reshape(X_permuted[:, :, i].shape)\n",
    "        \n",
    "        # CalculeazÄƒ performanÈ›a cu feature-ul amestecat\n",
    "        permuted_pred = model.predict(X_permuted, verbose=0)\n",
    "        permuted_mse = mean_squared_error(y_test[:n_samples].flatten(), permuted_pred.flatten())\n",
    "        \n",
    "        # ImportanÈ›a = cÃ¢t de mult se degradeazÄƒ performanÈ›a\n",
    "        importance = permuted_mse - baseline_mse\n",
    "        feature_importance[feature_name] = importance\n",
    "    \n",
    "    # SorteazÄƒ È™i afiÈ™eazÄƒ\n",
    "    sorted_importance = dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    print(f\"\\nğŸ† Top 10 features importante:\")\n",
    "    for i, (feature, importance) in enumerate(list(sorted_importance.items())[:10]):\n",
    "        print(f\"   {i+1:2d}. {feature:20s}: {importance:.8f}\")\n",
    "    \n",
    "    # Plot importanÈ›a\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    features = list(sorted_importance.keys())[:15]  # Top 15\n",
    "    importances = [sorted_importance[f] for f in features]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=importances,\n",
    "            y=features,\n",
    "            orientation='h',\n",
    "            marker=dict(color=importances, colorscale='Viridis')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"ImportanÈ›a Features (Top 15)\",\n",
    "        xaxis_title=\"Degradarea MSE\",\n",
    "        yaxis_title=\"Features\",\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return sorted_importance\n",
    "\n",
    "# AnalizÄƒ importanÈ›Äƒ features\n",
    "feature_importance = analyze_feature_importance_approximation(\n",
    "    model, X_test, y_test, feature_names, n_samples=200\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 13. Salvarea Modelului È™i ConfiguraÈ›iei\n",
    "\n",
    "# %%\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model_and_config(model, scalers, feature_names, results, config_info):\n",
    "    \"\"\"SalveazÄƒ modelul È™i toate configuraÈ›iile\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_name = f\"energy_lstm_model_{timestamp}\"\n",
    "    \n",
    "    print(f\"ğŸ’¾ Salvare model È™i configuraÈ›ie cu timestamp: {timestamp}\")\n",
    "    \n",
    "    # 1. Salvare model TensorFlow\n",
    "    model.save(f\"{base_name}.h5\")\n",
    "    print(f\"âœ… Model salvat: {base_name}.h5\")\n",
    "    \n",
    "    # 2. Salvare scalers\n",
    "    with open(f\"{base_name}_scalers.pkl\", 'wb') as f:\n",
    "        pickle.dump(scalers, f)\n",
    "    print(f\"âœ… Scalers salvaÈ›i: {base_name}_scalers.pkl\")\n",
    "    \n",
    "    # 3. Salvare configuraÈ›ie È™i rezultate\n",
    "    config = {\n",
    "        'model_info': {\n",
    "            'sequence_length': SEQUENCE_LENGTH,\n",
    "            'output_steps': OUT_STEPS,\n",
    "            'overlap_ratio': OVERLAP_RATIO,\n",
    "            'total_features': len(feature_names),\n",
    "            'training_samples': len(X_train),\n",
    "            'test_samples': len(X_test)\n",
    "        },\n",
    "        'feature_names': feature_names,\n",
    "        'performance_metrics': results['metrics'],\n",
    "        'training_config': {\n",
    "            'epochs_completed': len(history.history['loss']),\n",
    "            'final_train_loss': float(history.history['loss'][-1]),\n",
    "            'final_val_loss': float(history.history['val_loss'][-1]),\n",
    "            'best_val_loss': float(min(history.history['val_loss']))\n",
    "        },\n",
    "        'data_info': config_info,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    with open(f\"{base_name}_config.json\", 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(f\"âœ… ConfiguraÈ›ie salvatÄƒ: {base_name}_config.json\")\n",
    "    \n",
    "    return base_name\n",
    "\n",
    "# InformaÈ›ii despre date pentru salvare\n",
    "data_config = {\n",
    "    'source_file': my_file,\n",
    "    'total_data_points': len(df_data),\n",
    "    'energy_range': [float(df_data['E_tot'].min()), float(df_data['E_tot'].max())],\n",
    "    'temperature_range': [float(df_data['T'].min()), float(df_data['T'].max())],\n",
    "    'dominant_frequency_hz': float(dominant_freqs[0]) if len(dominant_freqs) > 0 else None,\n",
    "    'dominant_period_steps': float(1/dominant_freqs[0]) if len(dominant_freqs) > 0 and dominant_freqs[0] != 0 else None\n",
    "}\n",
    "\n",
    "# Salvare completÄƒ\n",
    "saved_model_name = save_model_and_config(model, scalers, feature_names, results, data_config)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 14. FuncÈ›ie pentru ÃncÄƒrcarea È™i Utilizarea Modelului Salvat\n",
    "\n",
    "# %%\n",
    "def load_trained_model(base_name):\n",
    "    \"\"\"ÃncarcÄƒ un model antrenat anterior\"\"\"\n",
    "    print(f\"ğŸ“‚ ÃncÄƒrcare model: {base_name}\")\n",
    "    \n",
    "    try:\n",
    "        # ÃncÄƒrcare model\n",
    "        model = tf.keras.models.load_model(f\"{base_name}.h5\")\n",
    "        print(f\"âœ… Model Ã®ncÄƒrcat\")\n",
    "        \n",
    "        # ÃncÄƒrcare scalers\n",
    "        with open(f\"{base_name}_scalers.pkl\", 'rb') as f:\n",
    "            scalers = pickle.load(f)\n",
    "        print(f\"âœ… Scalers Ã®ncÄƒrcaÈ›i\")\n",
    "        \n",
    "        # ÃncÄƒrcare configuraÈ›ie\n",
    "        with open(f\"{base_name}_config.json\", 'r') as f:\n",
    "            config = json.load(f)\n",
    "        print(f\"âœ… ConfiguraÈ›ie Ã®ncÄƒrcatÄƒ\")\n",
    "        \n",
    "        print(f\"ğŸ“Š Model info:\")\n",
    "        print(f\"   â”œâ”€â”€ Features: {config['model_info']['total_features']}\")\n",
    "        print(f\"   â”œâ”€â”€ Sequence length: {config['model_info']['sequence_length']}\")\n",
    "        print(f\"   â”œâ”€â”€ Output steps: {config['model_info']['output_steps']}\")\n",
    "        print(f\"   â”œâ”€â”€ Performance RÂ²: {config['performance_metrics']['r2']:.4f}\")\n",
    "        print(f\"   â””â”€â”€ Performance MAE: {config['performance_metrics']['mae']:.6f}\")\n",
    "        \n",
    "        return model, scalers, config\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Eroare la Ã®ncÄƒrcare: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Exemplu de utilizare (decomenteazÄƒ pentru a testa)\n",
    "# loaded_model, loaded_scalers, loaded_config = load_trained_model(saved_model_name)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 15. FuncÈ›ie pentru PredicÈ›ii pe Date Noi\n",
    "\n",
    "# %%\n",
    "def predict_energy_sequence(model, scalers, feature_names, new_data, sequence_length):\n",
    "    \"\"\"\n",
    "    FuncÈ›ie pentru predicÈ›ii pe date complet noi\n",
    "    \n",
    "    Args:\n",
    "        model: Modelul antrenat\n",
    "        scalers: Scalers pentru normalizare\n",
    "        feature_names: Lista cu numele features\n",
    "        new_data: DataFrame cu date noi (trebuie sÄƒ aibÄƒ coloanele: E_tot, T, P)\n",
    "        sequence_length: Lungimea secvenÈ›ei de input\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”® PredicÈ›ii pe {len(new_data)} puncte noi...\")\n",
    "    \n",
    "    try:\n",
    "        # VerificÄƒ cÄƒ datele au coloanele necesare\n",
    "        required_cols = ['E_tot', 'T', 'P']\n",
    "        if not all(col in new_data.columns for col in required_cols):\n",
    "            raise ValueError(f\"Datele trebuie sÄƒ conÈ›inÄƒ coloanele: {required_cols}\")\n",
    "        \n",
    "        # Recreate features similar cu training\n",
    "        energy_scaled = scalers['energy_scaler'].transform(new_data[['E_tot']]).flatten()\n",
    "        temp_scaled = scalers['temp_scaler'].transform(new_data[['T']]).flatten()\n",
    "        pressure_scaled = scalers['pressure_scaler'].transform(new_data[['P']]).flatten()\n",
    "        \n",
    "        # Features derivate\n",
    "        energy_diff = np.gradient(energy_scaled)\n",
    "        energy_diff2 = np.gradient(energy_diff)\n",
    "        \n",
    "        energy_ma5 = pd.Series(energy_scaled).rolling(window=5, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "        energy_ma10 = pd.Series(energy_scaled).rolling(window=10, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "        energy_ma20 = pd.Series(energy_scaled).rolling(window=20, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "        \n",
    "        energy_vol5 = pd.Series(energy_scaled).rolling(window=5, center=True).std().fillna(0)\n",
    "        energy_vol10 = pd.Series(energy_scaled).rolling(window=10, center=True).std().fillna(0)\n",
    "        \n",
    "        temp_diff = np.gradient(temp_scaled)\n",
    "        temp_ma10 = pd.Series(temp_scaled).rolling(window=10, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
    "        \n",
    "        # Features Fourier (aproximare - ar trebui sÄƒ fie calculate similar cu training)\n",
    "        fourier_features, _ = create_fourier_features(energy_scaled, dominant_freqs, top_n=3)\n",
    "        \n",
    "        # Combine toate features\n",
    "        all_features = np.column_stack([\n",
    "            energy_scaled, temp_scaled, pressure_scaled,\n",
    "            energy_diff, energy_diff2,\n",
    "            energy_ma5, energy_ma10, energy_ma20,\n",
    "            energy_vol5, energy_vol10,\n",
    "            temp_diff, temp_ma10,\n",
    "            fourier_features\n",
    "        ])\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # PredicÈ›ii pentru fiecare secvenÈ›Äƒ posibilÄƒ\n",
    "        for i in range(len(all_features) - sequence_length + 1):\n",
    "            seq = all_features[i:i + sequence_length].reshape(1, sequence_length, -1)\n",
    "            pred = model.predict(seq, verbose=0)\n",
    "            predictions.append(pred[0])\n",
    "        \n",
    "        # Scalare inversÄƒ\n",
    "        predictions = np.array(predictions)\n",
    "        predictions_original = scalers['energy_scaler'].inverse_transform(\n",
    "            predictions.reshape(-1, 1)\n",
    "        ).reshape(predictions.shape)\n",
    "        \n",
    "        print(f\"âœ… {len(predictions)} predicÈ›ii generate\")\n",
    "        \n",
    "        return predictions_original\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Eroare la predicÈ›ie: {e}\")\n",
    "        return None\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 16. Rezumat Final È™i RecomandÄƒri\n",
    "\n",
    "# %%\n",
    "print(\"ğŸ¯ REZUMAT FINAL - ANALIZA TIME SERIES ENERGIE CINETICÄ‚\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nğŸ“Š PERFORMANÈšA MODELULUI:\")\n",
    "print(f\"   â”œâ”€â”€ RÂ² Score: {results['metrics']['r2']:.6f}\")\n",
    "print(f\"   â”œâ”€â”€ Mean Absolute Error: {results['metrics']['mae']:.8f}\")\n",
    "print(f\"   â”œâ”€â”€ Root Mean Square Error: {results['metrics']['rmse']:.8f}\")\n",
    "print(f\"   â””â”€â”€ Mean Absolute Percentage Error: {results['metrics']['mape']:.4f}%\")\n",
    "\n",
    "print(f\"\\nğŸ”§ CONFIGURAÈšIA OPTIMALÄ‚:\")\n",
    "print(f\"   â”œâ”€â”€ Sequence Length: {SEQUENCE_LENGTH} paÈ™i (2Ã— perioada principalÄƒ)\")\n",
    "print(f\"   â”œâ”€â”€ Output Steps: {OUT_STEPS} paÈ™i (0.5Ã— perioada)\")\n",
    "print(f\"   â”œâ”€â”€ Total Features: {len(feature_names)}\")\n",
    "print(f\"   â”œâ”€â”€ Overlap Ratio: {OVERLAP_RATIO} ({int(OVERLAP_RATIO*100)}%)\")\n",
    "print(f\"   â””â”€â”€ Training Samples: {len(X_train):,}\")\n",
    "\n",
    "print(f\"\\nğŸŒŠ ANALIZA FOURIER:\")\n",
    "if len(dominant_freqs) > 0:\n",
    "    print(f\"   â”œâ”€â”€ FrecvenÈ›a dominantÄƒ: {dominant_freqs[0]:.6f} Hz\")\n",
    "    print(f\"   â”œâ”€â”€ Perioada principalÄƒ: {1/dominant_freqs[0]:.2f} paÈ™i\")\n",
    "    print(f\"   â””â”€â”€ Features Fourier generate: {len([f for f in feature_names if 'sin_' in f or 'cos_' in f])}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ RECOMANDÄ‚RI PENTRU OPTIMIZARE ULTERIOARÄ‚:\")\n",
    "print(f\"   â”œâ”€â”€ ExperimenteazÄƒ cu arhitecturi Transformer pentru capturarea dependinÈ›elor\")\n",
    "print(f\"   â”œâ”€â”€ ÃncearcÄƒ ensemble methods cu multiple modele LSTM\")\n",
    "print(f\"   â”œâ”€â”€ ConsiderÄƒ attention mechanisms pentru features importante\")\n",
    "print(f\"   â”œâ”€â”€ TesteazÄƒ data augmentation prin rotaÈ›ii de fazÄƒ\")\n",
    "print(f\"   â””â”€â”€ ExploreazÄƒ autoencoder pentru detecÈ›ia anomaliilor\")\n",
    "\n",
    "print(f\"\\nğŸ“ FIÈ˜IERE SALVATE:\")\n",
    "print(f\"   â”œâ”€â”€ Model: {saved_model_name}.h5\")\n",
    "print(f\"   â”œâ”€â”€ Scalers: {saved_model_name}_scalers.pkl\")\n",
    "print(f\"   â””â”€â”€ Config: {saved_model_name}_config.json\")\n",
    "\n",
    "print(f\"\\nğŸš€ URMÄ‚TORII PAÈ˜I:\")\n",
    "print(f\"   1. TesteazÄƒ modelul pe date complet noi\")\n",
    "print(f\"   2. ImplementeazÄƒ monitoring Ã®n timp real\")\n",
    "print(f\"   3. OptimizeazÄƒ hiperparametrii cu Optuna/Hyperopt\")\n",
    "print(f\"   4. DezvoltÄƒ API pentru predicÈ›ii Ã®n producÈ›ie\")\n",
    "\n",
    "print(\"\\nâœ… ANALIZA COMPLETÄ‚ FINALIZATÄ‚!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
